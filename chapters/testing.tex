\chapter{Testing and Crash Reporting}

The Android and iOS versions of the application were implemented in parallel, mostly on my personal machine with Ubuntu 16.04 and my Nexus 5 phone with Android 6.0.1, following a verification and fine-tuning for iOS. The development phone for iOS was iPhone 5C running the latest version of iOS which was available at the time. 

I have also used the iPhone and Android emulators for development and to see how the application looks with different screen resolutions and OS versions (the latter is especially relevant for Android). However, having access to real devices was crucial, since only a real device can give a feeling of how well touchable elements respond to touches, how the software keyboard influences the displayed content and how the application functions in terms of performance. A real iPhone device was also needed for implementing the native module for creating jobs, since the background capabilities of NSURLSession of an iPhone emulator do not fully correspond to the behavior of a real device.

All throughout the development, the application was receiving its data from Memsource servers, ie. I have not used any server implementation specifically for the development.


\section{Unit Testing}
The lowest level upon which the application is tested is unit testing. I have chosen Jest \footnote{http://facebook.github.io/jest/} for implementing the unit tests. Jest, like several other tools I have used, is a library actively developed by Facebook and is open source. 

It offers essential functionality similar to other popular Javascript test runners (e.g. AVA or Mocha), such as making assertions upon the results of tested code, creating mocks and also offers snapshot testing, which is functionality that can be used for testing the structure of React trees (or any serializable values) without directly rendering them. Snapshot testing is a very useful feature especially in React Native as it allows to test component appearance without the need for rendering the UI on a device or emulator. 

Jest creates a snapshot that captures the necessary information for component rendering. When the component changes, the snapshot changes as well, and we're notified of this fact during testing. If the change is intentional, we can simply overwrite the previous snapshot with a new one, or fix the problem if the change is a bug. We also see the change in version control when the it is being merged since the snapshot files live alongside the code. Snapshot testing also can be used for testing application state.

Snapshot testing currently has the drawback of not being able to trigger and capture possible changes in the inner state of the component (if there is any state), ie. snapshot testing only considers the component’s props. This, however, is a subject to change in one of the future releases of Jest.

Since a React Native app is a native application, we can use the same testing frameworks that we would use for testing any other native app on ios or Android. 

todo popsat kolik jich ej jak jsem je pouzil atd - vystupy

% TODO heuristic evaluation

\section{Testing With Users}

I performed informal testing of the Android application with three users all of whom were Android users. One was a user knowledgeable of Memsource in general but not its advanced features, second was a Memsource power user and third was a novice user. They were given a sheet with tasks (available in \autoref{chap:walkthrough}) they were asked to perform using the mobile application.

During the test I was, for the most part, just silently observing the user actions but also communicated with user when they wanted to tell me something. This section contains the outputs of the tests with my comments included. I also made a summary of the encountered problems.


\subsubsection{First Test (knowledgeable user)}

In the first test, all tasks were completed. The test revealed some inconsistencies in how project metadata is presented in the project info screen. User also reported that the control for selecting linguist (in the screen for creating new jobs) doesn’t seem to be a control that would react to touch (button), plus the heading which reads "select linguists" is not intuitive. 

Next, I observed that the user expected something (item selection) to happen when they tapped an item in the job list. The user got around it by long-pressing the item, which is the intended way of selecting it. The selection logic was implemented at a time when there was another action expected to happen upon short tap (a preview of the imported job). This functionality was decided not to be needed in the end. I have later implemented a fix for this - the short as well as long taps are now used for item selection. 


When selecting Term Bases (TBs) and Translation Memories (TMs), the user was slightly confused by the term "Selected TMs". I agree a better term would be "Attached TMs" as was suggested, but chose the term "Selected TMs" to keep consistency with Memsource Cloud. The user also found the screen for adding TMs and TBs generally confusing. This is due to too many controls being displayed on one screen (a control for filtering the results, control for selecting the target language and workflow step for the assignment, along with checkboxes and dialogs for selecting different parameters). This problem was later addressed by splitting the adding of TMs and TBs into two screens, so it now involves an extra step, but is much clearer.


\subsubsection{Second Test (power user)}

In the second test, all tasks were completed as well. The second user performed the project and job tasks successfully. They also found an unexpected text reading "owner from different organization" on some project items displayed in the project list. This is an older issue caused by the API not returning the project owner. It remained in the application since I forgot to investigate the root cause. At any rate, the message looks confusing and will be removed and I will look more into the cause and possible solutions. The user also requested one feature and noticed one bug, both relating to content displayed in the project info screen.

Similar to the first test, I observed the user was confused when adding TMs because of they expected that the TMs in the screen that lists them to react to touches, while the listing is intended as a read-only preview. I later addressed this problem by adding an "edit" button in the navigation bar but it still remains to be assessed whether this is a sufficient improvement.

The user also reported confusion over the fact that when TBs are added for language X, the "Selected TBs" section shows TBs selected also for other languages. This is something I need to think through and the solution to this perhaps lays at the server side which provides this data.

%Show date created in project info screen
%Workflow steps not always shown in proj info screen

\subsubsection{Third Test (novice)}

The third test was also completed successfully. The app already included the aforementioned fixes to how jobs are selected and how TM and TB selection is split into two steps which both made completing the tasks easier for the user. 

User had the following remarks to the application: when only one job is selected for editing, the edited fields read "leave unchanged" while they could read the actual data of the edited job, and "leave unchanged" should only be used when multiple jobs are being edited. This is certainly true and may be implemented. 

User also didn't like that the very general "no results" text is shown when an empty list of jobs, TMs or TBs is displayed. User would appreciate more specific messages. This is a simple to do improvement and will be implemented.

Another thing the user was confused about were the icons for searching (magnifying glass) and filtering (funnel). This is something not very easy to address, since searching and filtering are very similar, yet different tasks.

Same as in previous tests, user had complaints about how TM and TB selection works --- the user didn't like that some items are being selected and de-selected automatically.

My last observation from this test is that the user wasn't sure how to trigger the search after entering the search phrase in the text box. This is done through a key located in the lower right corner of the screen, which shows a magnifying glass. The icon, however, is rather small and therefore not always easy to notice. User found the button after a few seconds and managed to trigger the search but said they expected the search to happen as they type. This is something we will consider for later implementation.

\subsection{Test Conclusions}

The tests of the application for Android were completed by all users. Unfortunately I didn't manage to run a user test with the iOS version of the application. The tasks related to projects and jobs were completed by all users without problems. Issues emerged when attaching Translation Memories and Term Bases to the project. This is due to the way items are presented for selection. I have already addressed some of these issues and will continue working on them.

The way TMs and TBs are selected can be further improved. Now, while the mobile app is consistent with Memsource Cloud, I find it somewhat confusing (and the tests confirmed it): when the user selects eg. a TM in read more, write mode is automatically selected too and user is not always in complete control. Not all checkboxes that seem to be possible to select can be selected due to selection constraints.

I believe a better solution to the problem is a single button reading something like “select TM (TB) mode”, which would open a list with all possible modes for the TM (TB) and only one mode could be selected. The list would show all modes: “Read \& Write”, “Read only” in case of TMs and in case of TBs “Read \& Write”, “Read \& write \& QA”, “Read only”, “Read \& QA”. If applicable, the modes would be displayed along with an explanation of why a particular mode cannot be selected, or that selecting the mode will overwrite the mode of some other TM (TB). This would give user complete control and no unexpected automatic selecting or de-selecting happening as it is now.


\section{Crash Reporting}


One of the requirements is the ability to collect crash reports from users running the application so that we can observe how it functions on their devices and react to potential issues. 

There are several services which provide crash reporting as well as means for collecting information about how the app is used, similar to analytics as it is on the web. One of such solutions which is widely used and also has community-developed bindings for React Native is called Fabric \footnote{https://get.fabric.io/} and is provided by Twitter free of charge for both iOS and Android. Fabric's crash reporting service is called Crashlytics, and the analytics is marketed under the name Answers. I have chosen Fabric because of its popularity, the ability to use Crashlytics as well as Answers and the fact it is available for both platforms. I have incorporated Fabric into both Android and iOS versions of the app. 

While Fabric with the community-developed module for React Native does report the application crashes, I am not completely happy with how the reported issues are presented in the Crashlytics dashboard. In particular, when a crash happens in the Javascript layer of the application, the information is merely passed to the native module which backs the Javascript module. In the native module, an exception is thrown and its information is collected and recorded by Crashlytics. The problem with this approach is that all exceptions are thrown from exactly the same line in code and Crashlytics considers all of them to relate to a single bug. Then, in the Crashlytics dashboard (which is best accessible though web browser) all of the exceptions are grouped together which makes working with Crashlytics uncomfortable. I have investigated other possible crash reporting solutions and found Bugsnag \footnote{https://bugsnag.com/} which directly supports React Native. It, however, does not provide any analytics service and is not offered free of charge. I will evaluate the situation around bug reporting after we gain more experience with Crashlytics and might make a switch.